---
title: "Text Analysis Project"
author: "Lee, Woo Chan"
date: "11/10/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(cluster)
library(factoextra)
library(tidyverse)
library(quanteda)
library(seededlda)
library(lubridate)
library(cmu.textstat)
```

# LDA (Topic modeling)
```{r}
# Load full twitter dataset grouped by tickers and quarterly (date)
twt_q <- read_csv("/Users/lee14257/Development/CMU/Text Analysis/Project/CBE2/twt_ticker_quarter.csv")

```
\

```{r}
# Create new column doc_id, which represents the ticker symbol + date
twt_q$doc_id <- paste0(twt_q$ticker_symbol, "_", twt_q$post_date)
twt_q <- twt_q %>%
  rename('text' = 'body')

```
\

```{r}
# Subset the MSFT and TSLA tickers
twt_msft <- twt_q %>% subset(ticker_symbol == "MSFT")
twt_tsla <- twt_q %>% subset(ticker_symbol == "TSLA")

```
\


## LDA for TSLA
```{r}
# Create token object for TSLA
twt_tsla_tkn <- twt_tsla %>%
  corpus() %>%
  tokens(what="fastestword", remove_punct = TRUE, remove_symbols = TRUE, 
          remove_numbers=TRUE, remove_url=TRUE, remove_separators=TRUE, 
          split_hyphens=TRUE  
           ) %>%
  tokens_remove(c('\\$[a-z0-9]+', '\\#[a-z0-9]+', '[0-9]+\\%', '\\@[a-z0-9]+'), 
                valuetype='regex') %>%
  tokens_remove(c(stopwords("english"), "tsla", "tesla", 
                  "tesla's", "btindle:", "200:1", "10:45", "w/code", "4x,", 
                  "5x,", "leech-boy"))

```
\

```{r}
# Create dfm for TSLA
# Define min_termfreq and max_termfreq to restrict dfm
twt_tsla_dfm <- twt_tsla_tkn %>% dfm() %>% 
              dfm_trim(min_termfreq = 30, max_termfreq = 85)

```
\

```{r}
# LDA for TSLA
set.seed(2023)
tsla_lda <- textmodel_lda(twt_tsla_dfm, k = 6)

# Overview of top 30 words for each topic
tsla_30 <- as.data.frame(terms(tsla_lda, 30))

```
\

```{r}
# Print top words for each topics
print(tsla_30)

# Store significant / relevant words in tsla_30 in vector form
topic_composition_tsla <- data.frame(topic_num = NA, words = NA)
topic_composition_tsla[1,] <- c("Topic 1", "[ deteriorating, priceclick, 
                                itb, supertrades, xle ]")
topic_composition_tsla[2,] <- c("Topic 4", "[ positivestocks, tesla/solarcity, 
                                merger, plan, hedges]")
topic_composition_tsla[3,] <- c("Topic 5", "[ mars, breached, bloodbath, 
                                falling, breakout ]")
topic_composition_tsla[4,] <- c("Topic 6", "[ laws, threats, embarassment, 
                                elon, statements ]")
```
\

```{r echo=FALSE}
# Key topics for TSLA
kableExtra::kbl(topic_composition_tsla, caption = "Topic Composition for TSLA", 
                booktabs = T, col.names=c("Topics", "Key Tokens"), 
                linesep = "") %>%
  kableExtra::kable_styling(latex_options = "HOLD_position") %>%
  kableExtra::kable_classic()

```
\

```{r}
# Assign each doc_id to the topics
data.frame(doc_id = twt_tsla$doc_id, Topic = topics(tsla_lda))
```
\

## Plot topic in the time series graph for TSLA
```{r}
# Read in docuscope-tagged dfm, and filter TSLA
tsla_docuscope <- read_csv("/Users/lee14257/Development/CMU/Text Analysis/Project/CBE2/twt_docuscope_normalized.csv") %>%
  filter(ticker == "TSLA")

```
\

```{r}
# Transform dfm to feed to ggplot
tsla_sentiment <- tsla_docuscope %>%
  mutate(
    ticker_symbol = str_extract(doc_id, "^[A-Z]+"),
    date = as.Date(paste0(word(doc_id, 2, sep = "_"), '-01'), format='%Y-%m-%d')
    ) %>%
  dplyr::select(ticker_symbol, date, sentiment_score) %>%
  filter(date >= "2015-01-01")

```
\

```{r, fig.height=4, fig.width=7, fig.cap="Sentiment scores for TSLA from 2015 ~ 2019"}
# Graphing the time series plot for TSLA
ggplot(tsla_sentiment, aes(x=date, y=sentiment_score)) +
    geom_point(size = .5) +
    geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), size=1, 
                level=0.95, se=T, colour="darkgreen") +
    labs(x="Date", y = "Sentiment Score", 
         title="Sentiment scores for TSLA over time (2015~2020)")+ 
    theme(panel.grid.minor.x=element_blank(),
          panel.grid.major.x=element_blank()) +
    theme(panel.grid.minor.y=element_blank(),
          panel.grid.major.y=element_line(colour = "gray",size=0.25)) +
    theme(rect = element_blank()) +
    theme(legend.title=element_blank()) + 
    geom_vline(xintercept = c(ymd("2016/06/30"),
                              ymd("2017/06/30"), 
                              ymd("2018/06/30")), linetype = 2)
  
```
\

## LDA for MSFT
```{r}
# Create token for MSFT
twt_msft_tkn <- twt_msft %>%
  corpus() %>%
  tokens(what="fastestword", remove_punct = TRUE, remove_symbols = TRUE, 
          remove_numbers=TRUE, remove_url=TRUE, remove_separators=TRUE, 
          split_hyphens=TRUE  
           ) %>%
  tokens_remove(c('\\$[a-z0-9]+', '\\#[a-z0-9]+', '[0-9]+\\%', '\\@[a-z0-9]+'), 
                valuetype='regex') %>%
  tokens_remove(c(stopwords("english"), "microsoft", "microsoft's", "msft"))

```
\

```{r}
# Create dfm for msft
twt_msft_dfm <- twt_msft_tkn %>% dfm() %>% 
              dfm_trim(min_termfreq = 25, max_termfreq = 95)

```
\

```{r}
# LDA model
set.seed(222)
msft_lda <- textmodel_lda(twt_msft_dfm, k = 6)

# Overview of top 30 words for each topic
msft_30 <- as.data.frame(terms(msft_lda, 30))

```
\

```{r}
# Print top words for each topics
print(msft_30)

# Store significant / relevant words in msft_30 in vector form
topic_composition <- data.frame(topic_num = NA, words = NA)
topic_composition[1,] <- c("Topic 1", "[ opt, parent, partnered, outsells, 
                           valuation ]")
topic_composition[2,] <- c("Topic 2", "[ alzheimers, nowfunded, reuters, 
                           hyped, pattern ]")
topic_composition[3,] <- c("Topic 3", "[ racist, apologizes, appeals, 
                           sues, grants ]")
topic_composition[4,] <- c("Topic 4", "[ ban, crackdown, vulnerabilities, 
                           africa, chromebook ]")
topic_composition[5,] <- c("Topic 5", "[ revitalize, analyze, declining, 
                           saturday, roundup ]")
topic_composition[6,] <- c("Topic 6", "[ slack, battlefield, covers, 
                           transformation, crispr ]")

```
\

```{r echo=FALSE}
# Key topics for MSFT 
kableExtra::kbl(topic_composition, caption = "Cluster Composition", 
                booktabs = T, col.names=c("Topics", "Key Tokens"), 
                linesep = "") %>%
  kableExtra::kable_styling(latex_options = "HOLD_position") %>%
  kableExtra::kable_classic()

```

```{r}
# Assign each doc_id to the topics
data.frame(doc_id = twt_msft$doc_id, Topic = topics(msft_lda))

```
\

```{r}
# Load docuscope-tagged dfm for MSFT
msft_docuscope <- read_csv("/Users/lee14257/Development/CMU/Text Analysis/Project/CBE2/twt_docuscope_normalized.csv") %>%
  filter(ticker == "MSFT")

```
\

```{r}
# Transform dfm to feed to ggplot
msft_sentiment <- msft_docuscope %>%
  mutate(
    ticker_symbol = str_extract(doc_id, "^[A-Z]+"),
    date = as.Date(paste0(word(doc_id, 2, sep = "_"), '-01'), format='%Y-%m-%d')
    ) %>%
  dplyr::select(ticker_symbol, date, sentiment_score) %>%
  filter(date >= "2015-01-01")

```
\

```{r sentiment_score, fig.height=4, fig.width=7, fig.cap="Sentiment scores for MSFT from 2015 ~ 2019"}
# Graphing the time series plot for MSFT
ggplot(msft_sentiment, aes(x=date, y=sentiment_score)) +
    geom_point(size = .5) +
    geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), size=1, 
                level=0.95, se=T, colour="red") +
    labs(x="Date", y = "Sentiment Score", title="Sentiment scores for MSFT 
         over time (2015~2020)")+ 
    theme(panel.grid.minor.x=element_blank(),
          panel.grid.major.x=element_blank()) +
    theme(panel.grid.minor.y=element_blank(),
          panel.grid.major.y=element_line(colour = "gray",size=0.25)) +
    theme(rect = element_blank()) +
    theme(legend.title=element_blank()) + 
    geom_vline(xintercept = c(ymd("2016/06/30"),
                              ymd("2017/01/01"), ymd("2017/06/30"), 
                              ymd("2018/01/01"), ymd("2018/06/30")), 
               linetype = 2)

```
\

# Multidimension Analysis (TSLA vs MSFT)
```{r, warning=FALSE}
# Create docuscope-tagged, normalized dfm appropriate for MDA
twt_year <- read_csv("/Users/lee14257/Development/CMU/Text Analysis/Project/CBE2/twt_docuscope_normalized.csv") %>% 
  filter(ticker == 'TSLA' | ticker == 'MSFT') %>%
  mutate(
    ticker = as.factor(paste0(ticker, "_", year))
  ) %>% dplyr::select(-year, -sentiment_score, -citationhedged) %>%
  column_to_rownames("doc_id")

```
\

```{r, fig.height=4.3, fig.width=7, fig.cap="Scree plot of factors."}
# Scree plot to select optimum number of factors
screeplot_mda(twt_year)
```
\

```{r}
# Calculate factor loadings
twt_mda <- mda_loadings(twt_year, n_factors = 5)

```
\

```{r}
# Table for factor loadings in factor1, factor2 and factor3
knitr::kable(attr(twt_mda, 'loadings'), caption = 
               "Foctor loadings for midterm corpus", booktabs = T, 
             linesep = "", digits = 2)

```
\

```{r}
# Compare significance of the three factors
f1_lm <- lm(Factor1 ~ group, data = twt_mda)
names(f1_lm$coefficients) <- names(coef(f1_lm)) %>% str_remove("group")
f2_lm <- lm(Factor2 ~ group, data = twt_mda)
names(f2_lm$coefficients) <- names(coef(f2_lm)) %>% str_remove("group")
f3_lm <- lm(Factor3 ~ group, data = twt_mda)
names(f3_lm$coefficients) <- names(coef(f3_lm)) %>% str_remove("group")
f4_lm <- lm(Factor4 ~ group, data = twt_mda)
names(f4_lm$coefficients) <- names(coef(f4_lm)) %>% str_remove("group")
f5_lm <- lm(Factor5 ~ group, data = twt_mda)
names(f5_lm$coefficients) <- names(coef(f5_lm)) %>% str_remove("group")

```
\

```{r results = 'asis', echo=FALSE}
# Output results with DF, R squared and F statistics for each factors
jtools::export_summs(f1_lm, f2_lm, f3_lm, f4_lm, f5_lm, statistics = 
                       c(DF = "df.residual", R2 = "r.squared", 
                         "F statistic" = "statistic"), model.names = 
                       c("Factor 1", "Factor 2", "Factor 3", "Factor 4", 
                         "Factor 5"), error_format = "",
  error_pos = "same")

```
\

```{r fig.cap="Dimension score means by discipline plotted along Factor 1."}
# Heatmap for factor 1 (chosen)
mda.biber::heatmap_mda(twt_mda, n_factor = 1)

```
\

\pagebreak

# Code Appendix (part 2) - Topic Modeling, Multidimensional Analysis

```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```